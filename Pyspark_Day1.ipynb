{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sparkcontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext:\n",
    "\n",
    "It is the entry point to Spark's functionality. It is responsible for connecting to a Spark cluster, initializing the cluster resources, and coordinating the execution of Spark jobs.\n",
    "\n",
    "Role: Manages the Spark applicationâ€™s environment and communicates with the cluster manager. It provides the fundamental methods for creating RDDs (Resilient Distributed Datasets) and executing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext object\n",
    "sc = SparkContext(appName=\"MySparkApplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Reyansh.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApplication</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MySparkApplication>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Reyansh.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApplication</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MySparkApplication>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD-Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an RDD from a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Alice\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements of the rdd:  [('Alice', 25), ('Bob', 30), ('Charlie', 35), ('Alice', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "print(\"All elements of the rdd: \", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs Operation: Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n"
     ]
    }
   ],
   "source": [
    "# Count action: Count the number of elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of the rdd:  ('Alice', 25)\n"
     ]
    }
   ],
   "source": [
    "# First action: Retrieve the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two elements of the rdd:  [('Alice', 25), ('Bob', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Take action: Retrieve the n elements of the RDD\n",
    "taken_elements = rdd.take(2)\n",
    "print(\"The first two elements of the rdd: \", taken_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach action: Print each element of the RDD\n",
    "rdd.foreach(lambda x: print(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd with uppercease name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]\n"
     ]
    }
   ],
   "source": [
    "result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercease name: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs Operation: Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYSPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession:\n",
    "\n",
    "Introduced in Spark 2.0, SparkSession is a unified entry point for reading data, working with DataFrames, and executing SQL queries.\n",
    "\n",
    "Role: It encapsulates SparkContext and provides a high-level API for data processing. It includes functionality for working with structured data and allows you to use DataFrame and SQL APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# Print PySpark version\n",
    "print(f\"PySpark Version: {pyspark.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java Version: java version \"1.8.0_391\"\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get the Java version\n",
    "def get_java_version():\n",
    "    try:\n",
    "        version_info = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "        version_info = version_info.decode('utf-8').split('\\n')[0]  # Extract the version line\n",
    "        print(f\"Java Version: {version_info}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Java is not installed or not found in the PATH.\")\n",
    "\n",
    "get_java_version()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spark session\n",
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/krishnaik06/Pyspark-With-Python/blob/main/Tutorial%204-%20Pyspark%20Dataframes-%20Filter%20operation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Reyansh.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x128c8a10a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n",
    "#sparksession created.checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create sata frame\n",
    "#df_pyspark = spark.createDataFrame(Data)\n",
    "#Data =[....]\n",
    "\n",
    "## read the dataset\n",
    "df_pyspark=spark.read.option('header','true').csv('C:/Users/rammo/OneDrive/Desktop/Book1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+\n",
      "|_c0|     Name| age|Experience|Salary|\n",
      "+---+---------+----+----------+------+\n",
      "|  1|     John|  32|         5| 20000|\n",
      "|  2|    Krish|  31|        10| 30000|\n",
      "|  3|Sudhanshu|  30|         8| 25000|\n",
      "|  4|    Sunny|  29|         4| 20000|\n",
      "|  5|     Paul|  24|         3| 20000|\n",
      "|  6|   Harsha|  21|         1| 15000|\n",
      "|  7|  Shubham|  23|         2| 18000|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|\n",
      "|  9|     NULL|  34|        10| 38000|\n",
      "| 10|     NULL|  36|      NULL|  NULL|\n",
      "+---+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, Name='John', age=32, Experience=5, Salary=20000)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     NAME|EXPERIENCE|\n",
      "+---------+----------+\n",
      "|     John|         5|\n",
      "|    Krish|        10|\n",
      "|Sudhanshu|         8|\n",
      "|    Sunny|         4|\n",
      "|     Paul|         3|\n",
      "|   Harsha|         1|\n",
      "|  Shubham|         2|\n",
      "|   Mahesh|      NULL|\n",
      "|     NULL|        10|\n",
      "|     NULL|      NULL|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['NAME','EXPERIENCE']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'NAME'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['NAME'] #to check column avalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('age', 'int'),\n",
       " ('Experience', 'int'),\n",
       " ('Salary', 'int')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+-----------------+-----------------+-----------------+\n",
      "|summary|               _c0|  Name|              age|       Experience|           Salary|\n",
      "+-------+------------------+------+-----------------+-----------------+-----------------+\n",
      "|  count|                10|     8|                9|                8|                9|\n",
      "|   mean|               5.5|  NULL|28.88888888888889|            5.375|25111.11111111111|\n",
      "| stddev|3.0276503540974917|  NULL|5.158595846847387|3.543101950067402|8964.435905906801|\n",
      "|    min|                 1|Harsha|               21|                1|            15000|\n",
      "|    max|                10| Sunny|               36|               10|            40000|\n",
      "+-------+------------------+------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+-----------------------+\n",
      "|_c0|     Name| age|Experience|Salary|Experience After 2 year|\n",
      "+---+---------+----+----------+------+-----------------------+\n",
      "|  1|     John|  32|         5| 20000|                      7|\n",
      "|  2|    Krish|  31|        10| 30000|                     12|\n",
      "|  3|Sudhanshu|  30|         8| 25000|                     10|\n",
      "|  4|    Sunny|  29|         4| 20000|                      6|\n",
      "|  5|     Paul|  24|         3| 20000|                      5|\n",
      "|  6|   Harsha|  21|         1| 15000|                      3|\n",
      "|  7|  Shubham|  23|         2| 18000|                      4|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|                   NULL|\n",
      "|  9|     NULL|  34|        10| 38000|                     12|\n",
      "| 10|     NULL|  36|      NULL|  NULL|                   NULL|\n",
      "+---+---------+----+----------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding Columns in data frame\n",
    "df_pyspark=df_pyspark.withColumn('Experience After 2 year',df_pyspark['EXPERIENCE']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark Handling Missing Values\n",
    "*Dropping Columns.\n",
    "*Dropping Rows.\n",
    "*Various Parameter In Dropping functionalities.\n",
    "*Handling Missing values by Mean, MEdian And Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+\n",
      "|_c0|     Name| age|Experience|Salary|\n",
      "+---+---------+----+----------+------+\n",
      "|  1|     John|  32|         5| 20000|\n",
      "|  2|    Krish|  31|        10| 30000|\n",
      "|  3|Sudhanshu|  30|         8| 25000|\n",
      "|  4|    Sunny|  29|         4| 20000|\n",
      "|  5|     Paul|  24|         3| 20000|\n",
      "|  6|   Harsha|  21|         1| 15000|\n",
      "|  7|  Shubham|  23|         2| 18000|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|\n",
      "|  9|     NULL|  34|        10| 38000|\n",
      "| 10|     NULL|  36|      NULL|  NULL|\n",
      "+---+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop the columns\n",
    "df_pyspark=df_pyspark.drop('Experience After 2 year')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+\n",
      "|_c0| New Name| age|Experience|Salary|\n",
      "+---+---------+----+----------+------+\n",
      "|  1|     John|  32|         5| 20000|\n",
      "|  2|    Krish|  31|        10| 30000|\n",
      "|  3|Sudhanshu|  30|         8| 25000|\n",
      "|  4|    Sunny|  29|         4| 20000|\n",
      "|  5|     Paul|  24|         3| 20000|\n",
      "|  6|   Harsha|  21|         1| 15000|\n",
      "|  7|  Shubham|  23|         2| 18000|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|\n",
      "|  9|     NULL|  34|        10| 38000|\n",
      "| 10|     NULL|  36|      NULL|  NULL|\n",
      "+---+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+----------+------+\n",
      "|_c0|     Name|age|Experience|Salary|\n",
      "+---+---------+---+----------+------+\n",
      "|  1|     John| 32|         5| 20000|\n",
      "|  2|    Krish| 31|        10| 30000|\n",
      "|  3|Sudhanshu| 30|         8| 25000|\n",
      "|  4|    Sunny| 29|         4| 20000|\n",
      "|  5|     Paul| 24|         3| 20000|\n",
      "|  6|   Harsha| 21|         1| 15000|\n",
      "|  7|  Shubham| 23|         2| 18000|\n",
      "+---+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()\n",
    "#droping null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+----------+------+\n",
      "|_c0|     Name|age|Experience|Salary|\n",
      "+---+---------+---+----------+------+\n",
      "|  1|     John| 32|         5| 20000|\n",
      "|  2|    Krish| 31|        10| 30000|\n",
      "|  3|Sudhanshu| 30|         8| 25000|\n",
      "|  4|    Sunny| 29|         4| 20000|\n",
      "|  5|     Paul| 24|         3| 20000|\n",
      "|  6|   Harsha| 21|         1| 15000|\n",
      "|  7|  Shubham| 23|         2| 18000|\n",
      "+---+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any==how\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+\n",
      "|_c0|     Name| age|Experience|Salary|\n",
      "+---+---------+----+----------+------+\n",
      "|  1|     John|  32|         5| 20000|\n",
      "|  2|    Krish|  31|        10| 30000|\n",
      "|  3|Sudhanshu|  30|         8| 25000|\n",
      "|  4|    Sunny|  29|         4| 20000|\n",
      "|  5|     Paul|  24|         3| 20000|\n",
      "|  6|   Harsha|  21|         1| 15000|\n",
      "|  7|  Shubham|  23|         2| 18000|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|\n",
      "|  9|     NULL|  34|        10| 38000|\n",
      "| 10|     NULL|  36|      NULL|  NULL|\n",
      "+---+---------+----+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thresh=1:- This parameter specifies a threshold for dropping rows. It indicates \\nthat a row will be dropped if it has fewer than thresh non-null values.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##threshold\n",
    "df_pyspark.na.drop(how=\"any\",thresh=2).show() #Drops rows with fewer than 2 non-null values\n",
    "\"\"\"thresh=1:- This parameter specifies a threshold for dropping rows. It indicates \n",
    "that a row will be dropped if it has fewer than thresh non-null values.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+----------+------+\n",
      "|_c0|     Name|age|Experience|Salary|\n",
      "+---+---------+---+----------+------+\n",
      "|  1|     John| 32|         5| 20000|\n",
      "|  2|    Krish| 31|        10| 30000|\n",
      "|  3|Sudhanshu| 30|         8| 25000|\n",
      "|  4|    Sunny| 29|         4| 20000|\n",
      "|  5|     Paul| 24|         3| 20000|\n",
      "|  6|   Harsha| 21|         1| 15000|\n",
      "|  7|  Shubham| 23|         2| 18000|\n",
      "|  9|     NULL| 34|        10| 38000|\n",
      "+---+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Subset\n",
    "df_pyspark.na.drop(how=\"any\",subset=['EXPERIENCE']).show() #drop null values in subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filling the Missing Value\n",
    "df_pyspark.na.fill('Missing Values',['age','Experience'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+\n",
      "|_c0|     Name| age|Experience|Salary|\n",
      "+---+---------+----+----------+------+\n",
      "|  1|     John|  32|         5| 20000|\n",
      "|  2|    Krish|  31|        10| 30000|\n",
      "|  3|Sudhanshu|  30|         8| 25000|\n",
      "|  4|    Sunny|  29|         4| 20000|\n",
      "|  5|     Paul|  24|         3| 20000|\n",
      "|  6|   Harsha|  21|         1| 15000|\n",
      "|  7|  Shubham|  23|         2| 18000|\n",
      "|  8|   Mahesh|NULL|      NULL| 40000|\n",
      "|  9|     NULL|  34|        10| 38000|\n",
      "| 10|     NULL|  36|      NULL|  NULL|\n",
      "+---+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()\n",
    "# Replace null values in 'age' and 'Experience' columns with 'Missing Values'\n",
    "df_pyspark = df_pyspark.na.fill('Missing Values', subset=['age', 'Experience'])\n",
    "\n",
    "# Show updated DataFrame\n",
    "#df_pyspark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPUTATION \n",
    "to fill missing values.\n",
    "Imputation Requires Numeric Data:\n",
    "The Imputer class in PySpark is designed for numeric data. It computes summary statistics like the median or mean to fill in missing values, which requires numeric operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|_c0|Name     |age |Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|1  |John     |32  |5         |20000 |32         |5                 |20000         |\n",
      "|2  |Krish    |31  |10        |30000 |31         |10                |30000         |\n",
      "|3  |Sudhanshu|30  |8         |25000 |30         |8                 |25000         |\n",
      "|4  |Sunny    |29  |4         |20000 |29         |4                 |20000         |\n",
      "|5  |Paul     |24  |3         |20000 |24         |3                 |20000         |\n",
      "|6  |Harsha   |21  |1         |15000 |21         |1                 |15000         |\n",
      "|7  |Shubham  |23  |2         |18000 |23         |2                 |18000         |\n",
      "|8  |Mahesh   |NULL|NULL      |40000 |30         |4                 |40000         |\n",
      "|9  |NULL     |34  |10        |38000 |34         |10                |38000         |\n",
      "|10 |NULL     |36  |NULL      |NULL  |36         |4                 |20000         |\n",
      "+---+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n",
      "Imputation Strategy:  median\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Define the Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "# Fit the imputer model and transform the DataFrame\n",
    "imputer_model = imputer.fit(df_pyspark)\n",
    "df_imputed = imputer_model.transform(df_pyspark)\n",
    "\n",
    "# Show the DataFrame with the imputed columns\n",
    "df_imputed.show(truncate=False)\n",
    "\n",
    "# Display the imputation strategy and medians\n",
    "print(\"Imputation Strategy: \", imputer_model.getStrategy())\n",
    "#print(\"Medians for Imputation: \", imputer_model.medians)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYSPARK dataframes filteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+------+\n",
      "|_c0|Name|age|Experience|Salary|\n",
      "+---+----+---+----------+------+\n",
      "+---+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Salary of the people less than or equal to 20000\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows using Column expression\n",
    "filtered_df = df_pyspark.filter(col(\"SALARY\") >= 100000)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Filtering\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 30|\n",
      "|  2|  Bob| 25|\n",
      "|  3|Cathy| 29|\n",
      "|  4|David| 35|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=30),\n",
    "    Row(id=2, name=\"Bob\", age=25),\n",
    "    Row(id=3, name=\"Cathy\", age=29),\n",
    "    Row(id=4, name=\"David\", age=35)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 30|\n",
      "|  4|David| 35|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where age > 30\n",
    "filtered_df = df.filter(df['age'] > 29)\n",
    "\n",
    "# Show the result\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  2|  Bob| 25|\n",
      "|  3|Cathy| 29|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where age < 30\n",
    "filtered_df = df.where(df.age < 30)\n",
    "\n",
    "# Show the result\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 30|\n",
      "|  4|David| 35|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using SQL expression\n",
    "filtered_df = df.filter(\"age >= 30\")\n",
    "\n",
    "# Show the result\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark functions\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
